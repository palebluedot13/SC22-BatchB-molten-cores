{"backend_state":"running","connection_file":"/projects/afffd160-11c4-4b0e-8185-cf29c729cf98/.local/share/jupyter/runtime/kernel-3b685e6e-6743-405d-ba8d-ca6d2390991e.json","kernel":"ds_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1657220364817,"id":"038a62","input":"df.head()","kernel":"ds_env","pos":13,"scrolled":true,"start":1657220364791,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220365617,"id":"c94c1e","input":"px.bar(df.sort_values(by=[\"music_genre_name\"]), x='popularity', y='acousticness', color='music_genre_name')\n\n# non danceable songs are of the highest acoustecness\n# danceability and popularity are not correlated\n# for each popularity, the least danceable songs tend to have the highest acousticness (at least until about 45 popularity)***","kernel":"ds_env","pos":21,"start":1657220364854,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220366967,"id":"6f6337","input":"\n# # #prepocessing dataset\n# #X = df.loc[:, df.columns != 'music_genre']\n# #y = df.music_genre\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5, stratify=y)\n\n# # print(X_train.shape)\n# # print(X_test.shape)\n# # print(y_train.shape)\n# # print(y_test.shape)\n\n# scaler = preprocessing.StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.transform(X_test)\n\n#model building\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\n\ny_pred = model.predict(X_test_scaled)\n\nX_train_acc = model.score(X_train_scaled, y_train)\nprint(\"The Accuracy for Training Set is {}\".format(X_train_acc*100))\ntest_acc = model.score(X_test_scaled, y_test)\nprint(\"The Accuracy for Test Set is {}\".format(test_acc*100))\nprint(classification_report(y_test, y_pred))\n\n#Confusion matrixa\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g', xticklabels=labels, yticklabels=labels)\n","kernel":"ds_env","pos":32,"start":1657220365682,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220367730,"id":"934ab6","input":"gnb = GaussianNB()\ny_hat = gnb.fit(X_train, y_train).predict(X_test)\n\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g', xticklabels=labels, yticklabels=labels)","kernel":"ds_env","pos":35,"start":1657220366986,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220367810,"id":"59eddb","input":"dummies = pd.get_dummies(df[\"mode\"])\ndummies.head()","kernel":"ds_env","pos":40,"start":1657220367801,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220368016,"id":"7c8222","input":"target = df[\"music_genre\"]                                                                      # The target is defined.\nx_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=1) # Splits our data using the train_test_split() function on Sklearn.","kernel":"ds_env","pos":45,"start":1657220368016,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220368017,"id":"8bbf1b","input":"tst = RandomForestClassifier(max_depth=2, random_state=0)   # RandomForestRegressor is added.\ntst.fit(x_train, y_train)                                   # Fits the model.\n\ny_pred1 = tst.predict(x_test)                                  # Predicts.\n\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred1))","kernel":"ds_env","pos":47,"start":1657220368017,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220368017,"id":"f7f551","input":"dfxg = music_genre.copy(deep=True)\n\n#dropping irrelevent columns\nbadInfo = [\"instance_id\",\"obtained_date\",\"artist_name\",\"track_name\"]\ndfxg.drop(columns=badInfo,axis=1,inplace=True)\n\n#dropping null rows\ndfxg.dropna(inplace=True)\ndfxg.reset_index(drop=True, inplace=True) #Very good practice to reset how your rows are counted when you drop rows.\n\n#fixing tempo\ndfxg[\"tempo\"]=df[\"tempo\"].replace(\"?\",np.nan)\ndfxg[\"tempo\"] = df[\"tempo\"].astype(\"float\")\ndfxg[\"tempo\"]=df.groupby(\"music_genre\")[\"tempo\"].transform(lambda x: x.fillna(x.mean(skipna=True)))\n\n#fixing duration\ndfxg[\"duration_ms\"]=df[\"duration_ms\"].replace(-1.0,np.nan)\ndfxg[\"duration_ms\"]=df.groupby(\"music_genre\")[\"duration_ms\"].transform(lambda x: x.fillna(x.mean(skipna=True)))\ndfxg['duration_ms'] = np.around(df['duration_ms'],2)\n\n#creating dummy variables for categorical variables\ndummies = pd.get_dummies(dfxg[\"key\"])\ndfxg[\"A\"] = dummies[\"A\"]\ndfxg[\"B\"] = dummies[\"B\"]\ndfxg[\"C\"] = dummies[\"C\"]\ndfxg[\"D\"] = dummies[\"D\"]\ndfxg[\"E\"] = dummies[\"E\"]\ndfxg[\"F\"] = dummies[\"F\"]\ndfxg[\"G\"] = dummies[\"G\"]\ndfxg[\"A#\"] = dummies[\"A#\"]\ndfxg[\"C#\"] = dummies[\"C#\"]\ndfxg[\"D#\"] = dummies[\"D#\"]\ndfxg[\"F#\"] = dummies[\"F#\"]\ndfxg[\"G#\"] = dummies[\"G#\"]\ndfxg.drop(\"key\", axis = 1, inplace = True)\n\ndummies = pd.get_dummies(dfxg[\"mode\"])\ndfxg[\"Major\"] = dummies[\"Major\"]\ndfxg[\"Minor\"] = dummies[\"Minor\"]\ndfxg.drop(\"mode\", axis = 1, inplace = True)\n\ndummies = pd.get_dummies(dfxg[\"music_genre\"])\ndfxg[\"Electronic\"] = dummies[\"Electronic\"]\ndfxg[\"Anime\"] = dummies[\"Anime\"]\ndfxg[\"Jazz\"] = dummies[\"Jazz\"]\ndfxg[\"Alternative\"] = dummies[\"Alternative\"]\ndfxg[\"Country\"] = dummies[\"Country\"]\ndfxg[\"Rap\"] = dummies[\"Rap\"]\ndfxg[\"Blues\"] = dummies[\"Blues\"]\ndfxg[\"Rock\"] = dummies[\"Rock\"]\ndfxg[\"Classical\"] = dummies[\"Classical\"]\ndfxg[\"Hip-Hop\"] = dummies[\"Hip-Hop\"]\ndfxg.drop(\"music_genre\", axis = 1, inplace = True)\n\n#XGBoost model\ngenre_list = ['Electronic', 'Anime', 'Jazz', 'Alternative', 'Country', 'Rap', 'Blues', 'Rock', 'Classical', 'Hip-Hop']\ntarget = pd.get_dummies(dummies)\ninput_columns = dfxg.iloc[:,:-10]\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, train_size=0.8)\nscaler = preprocessing.StandardScaler().fit(x_train)\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\nmodel = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\nmodel.fit(x_train_scaled, y_train)\n\ny_pred = model.predict(x_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Overall Accuracy: \", accuracy)\n\ntotal_squared_error = (np.sum((y_test - y_pred)**2))\nmean_squared_error = total_squared_error/len(y_test)\nprint(mean_squared_error)\n\n#confusion matrix\nlabels = ['Electronic', 'Anime', 'Jazz', 'Alternative', 'Country', 'Rap', 'Blues', 'Rock', 'Classical', 'Hip-Hop']\nsns.heatmap(confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1)), annot=True, fmt='g', xticklabels=labels, yticklabels=labels)","kernel":"ds_env","pos":53,"start":1657220368017,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220692420,"exec_count":214,"id":"c34954","input":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.datasets import load_iris\nfrom IPython.display import Image\nfrom subprocess import call\nfrom sklearn import tree\nimport copy\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport xgboost as xgb\n\n#for Naïve Bayes\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import make_scorer, f1_score, accuracy_score, confusion_matrix, classification_report\nfrom xgboost import XGBClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation","kernel":"ds_env","pos":7,"start":1657220692408,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220757300,"exec_count":222,"id":"1dee56","input":"music_genre = pd.read_csv(\"music_genre.csv\")\ndf = music_genre.copy(deep=True)\n\ndf = copy.deepcopy(music_genre)","kernel":"ds_env","pos":11,"start":1657220756875,"state":"done","type":"cell"}
{"cell_type":"code","end":1657220759239,"exec_count":223,"id":"5fc9dd","input":"#dropping columns\nbadInfo = [\"instance_id\",\"obtained_date\",\"artist_name\",\"track_name\"]\ndf.drop(columns=badInfo,axis=1,inplace=True)\n#dropping null rows\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True) #Very good practice to reset how your rows are counted when you drop rows.\n\n#fixing tempo\ndf[\"tempo\"]=df[\"tempo\"].replace(\"?\",np.nan)\ndf[\"tempo\"] = df[\"tempo\"].astype(\"float\")\ndf[\"tempo\"]=df.groupby(\"music_genre\")[\"tempo\"].transform(lambda x: x.fillna(x.mean(skipna=True)))\ndf['tempo'] = np.around(df['tempo'],2)\n\n#fixing duration\ndf[\"duration_ms\"]=df[\"duration_ms\"].replace(-1.0,np.nan)\ndf[\"duration_ms\"]=df.groupby(\"music_genre\")[\"duration_ms\"].transform(lambda x: x.fillna(x.mean(skipna=True)))\ndf['duration_ms'] = np.around(df['duration_ms'],2)\n\n#change the values from string to int\ndf['key'] = stringToInt(df,'key')\ndf['mode'] = stringToInt(df,'mode')\ndf['music_genre_name'] = df['music_genre']\ndf['music_genre'] = stringToInt(df,'music_genre')\n\npio.templates.default = \"plotly_dark\"","kernel":"ds_env","output":{"0":{"name":"stdout","text":"{'A#': 0, 'D': 1, 'G#': 2, 'C#': 3, 'F#': 4, 'B': 5, 'G': 6, 'F': 7, 'A': 8, 'C': 9, 'E': 10, 'D#': 11} \n\n{'Minor': 0, 'Major': 1} \n\n{'Electronic': 0, 'Anime': 1, 'Jazz': 2, 'Alternative': 3, 'Country': 4, 'Rap': 5, 'Blues': 6, 'Rock': 7, 'Classical': 8, 'Hip-Hop': 9} \n\n"}},"pos":12,"start":1657220758979,"state":"done","type":"cell"}
{"cell_type":"code","id":"650e22","input":"","pos":56,"type":"cell"}
{"cell_type":"markdown","id":"030963","input":"# ____________________________________________________________________________________________________\n\n","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"06bf11","input":"# ____________________________________________________________________________________________________\n\n","pos":38,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"099ab3","input":"## Cleaning data frame","pos":9,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"165038","input":"# ____________________________________________________________________________________________________","pos":57,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1e9d0d","input":"# ____________________________________________________________________________________________________\n\n","pos":5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"329a49","input":"## Plot\n\n","pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"33a848","input":"# ____________________________________________________________________________________________________\n\n","pos":8,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"36b999","input":"##### [Markdown Guide cheat Sheet](https://www.markdownguide.org/cheat-sheet/)\n\n","pos":4,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"474e2e","input":"## Naive Bayes \nAbout  Naive Bayes :\nNative Bayes is a machine learning model for classification that uses the Bayes Theorem(\t<img src=\"https://s0.wp.com/latex.php?latex=%5Ctextrm%7BP%28H+%5Ctextbar+E%29+%3D+%7D+%C2%A0%5Cfrac%7B%5Ctextrm%7B+P%28E+%5Ctextbar+H%29+%2A+P%28H%29%7D%7D+%7B%5Ctextrm%7BP%28E%29%7D%7D&bg=ffffff&fg=000&s=0&c=20201002\" alt=\"img\" width=\"200\"/>\t)\n\n\n  -  P(H) is the probability of hypothesis H being true. This is known as the prior probability.\n  -  P(E) is the probability of the evidence(regardless of the hypothesis).\n  -  P(E|H) is the probability of the evidence given that hypothesis is true.\n  -  P(H|E) is the probability of the hypothesis given that the evidence is there.\n\n\nGood at  predicting:\n- Classical\n- Anime\n\nBad at predicting:\n- everything else","pos":34,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"50532f","input":"# ____________________________________________________________________________________________________\n\n","pos":42,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"51ba4c","input":"## Notes:\n- ### instrumentalness: vocals in a track\n\n- ### speechiness: detects the pressents of vocal words in a track\n\n- ### music genre: 10 different types\n\t0. Electronic\n    1. Classical\n    2. Jazz\n    3. anime\n    4. Rock\n    5. country\n    6. Rap\n    7. Blues\n    8. Hip-Hop\n    9.Alternative\n\n# problems with table:\n\n- [x] negative duration\n- [x] missing tempos\n- [x] rows 10000-10005 are nan values for every column\n- [ ] negative loudness","pos":2,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"54f4e0","input":"# Music Genre Prediction: Molten Cores\n\n<img src=\"https://www.warcrafttavern.com/wp-content/uploads/2020/10/WoW-Classic-Molten-Core-Guide-1024x729.jpg\" alt=\"img\" width=\"800\"/>","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"5f3999","input":"## Neural Networks","pos":55,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6065a3","input":"## Logistic Regression  ","pos":31,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6c877e","input":"## Classification Trees\n\n","pos":39,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"7a27d8","input":"# ____________________________________________________________________________________________________\n\n","pos":24.5,"type":"cell"}
{"cell_type":"markdown","id":"8019de","input":"## Accuracy","kernel":"ds_env","pos":36,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"807a79","input":"## Random Forest ","pos":43,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"8db7d8","input":"## Xgboost","pos":52,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"927e9f","input":"# ____________________________________________________________________________________________________","pos":54,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a93d3f","input":"## All Libaries & Imports ","pos":6,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b61414","input":"# ____________________________________________________________________________________________________","pos":51,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c530eb","input":"# ____________________________________________________________________________________________________\n\n","pos":22,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d3ff73","input":"## Spiting and scaling data","pos":23,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"ef7213","input":"# ____________________________________________________________________________________________________\n\n","pos":19,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f3270e","input":"# ____________________________________________________________________________________________________\n\n","pos":47.125,"type":"cell"}
{"cell_type":"markdown","id":"fb664e","input":"# ____________________________________________________________________________________________________\n\n","pos":33,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fc484d","input":"**About Classification Trees:** \n\nDecision Trees \\(DTs\\) are a non\\-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n\n**Pros**\n\n- Simple to understand and to interpret. Trees can be visualized.\n- Requires little data preparation.\n- Able to handle both numerical and categorical data.\n- Possible to validate a model using statistical tests.\n\n**Cons**\n\n- Decision\\-tree learners can create over\\-complex trees that do not generalize the data well.\n- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated\n- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. \n- Decision tree learners create biased trees if some classes dominate.\n\n","pos":39.25,"type":"cell"}
{"end":1657220364848,"id":"4357f7","input":"df.tail()","kernel":"ds_env","pos":14,"scrolled":true,"start":1657220364821,"state":"done","type":"cell"}
{"end":1657220367786,"id":"9f17ab","input":"print((np.sum((y_test - y_hat)**2))/len(y_test) )#mean squared error\nprint(f1_score(y_test, y_hat, average=None))\nprint(classification_report(y_test, y_hat,target_names=[i+\":\" for i in labels]))","kernel":"ds_env","pos":37,"scrolled":true,"start":1657220367740,"state":"done","type":"cell"}
{"end":1657220367843,"id":"5c00a3","input":"#df.drop(['music_genre'], axis=1, inplace=True)","kernel":"ds_env","pos":41.75,"start":1657220367837,"state":"done","type":"cell"}
{"end":1657220367899,"id":"4a5c56","input":"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 80% training and 20% test","kernel":"ds_env","pos":41.984375,"start":1657220367898,"state":"done","type":"cell"}
{"end":1657220368015,"id":"6c9137","input":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","kernel":"ds_env","pos":41.99609375,"start":1657220368015,"state":"done","type":"cell"}
{"end":1657220368016,"id":"1a3dc7","input":"#df[df['key']==\"B\"]","kernel":"ds_env","pos":44,"start":1657220368016,"state":"done","type":"cell"}
{"end":1657220368016,"id":"b94ede","input":"print(x_train.shape)    # Prints out the shape of the variable.\nprint(x_test.shape)     # Prints out the shape of the variable.\nprint(y_train.shape)    # Prints out the shape of the variable.\nprint(y_test.shape)     # Prints out the shape of the variable.","kernel":"ds_env","pos":46,"start":1657220368016,"state":"done","type":"cell"}
{"end":1657220368016,"id":"f0232f","input":"fig, axe = plt.subplots(figsize=(30,30))\ntree.plot_tree(clf, ax = axe, fontsize=15)","kernel":"ds_env","pos":41.9990234375,"start":1657220368016,"state":"done","type":"cell"}
{"end":1657220368017,"id":"5079ee","input":"# Classification Tree ( Test )\n\n#target = df[\"music_genre\"]                                                                     # The target is defined.\n#x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=1) # Splits our data using the train_test_split() function on Sklearn.\n\n#tst1 = DecisionTreeClassifier(max_depth = 2, random_state = 0)\n#tst1.fit(x_train, y_train)\n\n#tst1.predict(x_test)\n\n#fig, axe = plt.subplots(figsize=(20,10))\n#tree.plot_tree(tst1, ax = axe, fontsize=15)","kernel":"ds_env","pos":48,"start":1657220368017,"state":"done","type":"cell"}
{"end":1657220368017,"id":"6f43b7","input":"#label_encoder = preprocessing.LabelEncoder()\n#df['popularity']= label_encoder.fit_transform(df['popularity'])\n#df['popularity'].unique()","kernel":"ds_env","pos":49,"start":1657220368017,"state":"done","type":"cell"}
{"end":1657220368017,"id":"8afef8","input":"iris = load_iris()\n\n# Model (can also use single decision tree)\nmodel = RandomForestClassifier(n_estimators=10)\n\n# Train\nmodel.fit(iris.data, iris.target)\n\n# Extract single tree\nestimator = model.estimators_[5]\n\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = iris.feature_names,\n                class_names = iris.target_names,\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n\n# Convert to png using system command (requires Graphviz)\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nImage(filename = 'tree.png')","kernel":"ds_env","pos":47.25,"start":1657220368017,"state":"done","type":"cell"}
{"end":1657220368017,"id":"a344ed","input":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\ntree.plot_tree(clf)\nplt.show()","kernel":"ds_env","pos":50,"start":1657220368017,"state":"done","type":"cell"}
{"end":1657220368017,"id":"bce32c","input":"target = df[\"music_genre\"]                                                                      # The target is defined.\nx_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=1)  # Splits our data using the train_test_split() function on Sklearn.\n\n# Model (can also use single decision tree)\nmodel = RandomForestClassifier(n_estimators=10)\n\n# Train\nmodel.fit(x_train, y_train)\n\n# Extract single tree\ndf1 = model.estimators_[5]\n\n# Export as dot file\nexport_graphviz(df1, out_file='tree.dot', rounded = True, proportion = False, precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nImage(filename = 'tree.png')","kernel":"ds_env","pos":47.5,"start":1657220368017,"state":"done","type":"cell"}
{"end":1657220394485,"id":"c0ea89","input":"df = df.sort_values(by=[\"key\"])\npx.bar(df, x='music_genre_name', y='tempo',color='key')\n\n# tempo is highest for classical\n# tempo is lowest for electronic\n# clear pattern in tempo per genre","kernel":"ds_env","pos":21.5,"start":1657220394198,"state":"done","type":"cell"}
{"end":1657220695573,"exec_count":215,"id":"44b5bf","input":"#functions\ndef stringToInt(dataFrame,col):\n    test = {}\n    for i in dict(enumerate(dataFrame[col].unique())).items():#is a dictionary of the keys and corespodening number\n        #makes it so the keys and values of the dictionary switch\n        test[i[1]]=i[0]\n    print(test,'\\n')\n    return dataFrame[col].map(test)","kernel":"ds_env","pos":10,"start":1657220695552,"state":"done","type":"cell"}
{"end":1657220768073,"exec_count":224,"id":"893791","input":"X = df.loc[:,df.columns[:-2]]#input_columns\ny= df['music_genre']#what we want\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nsc = preprocessing.StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform (X_test)\n\n#for heat map\nlabels = ['Electronic', 'Anime', 'Jazz', 'Alternative', 'Country', 'Rap', 'Blues', 'Rock', 'Classical', 'Hip-Hop']","kernel":"ds_env","pos":24,"start":1657220768031,"state":"done","type":"cell"}
{"end":1657220791158,"exec_count":227,"id":"8c6380","input":"clf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, y)","kernel":"ds_env","pos":41.998046875,"start":1657220789892,"state":"done","type":"cell"}
{"end":1657220796138,"exec_count":228,"id":"dc60c3","input":"y_pred = clf.predict(X_test)","kernel":"ds_env","output":{"0":{"name":"stderr","text":"/projects/afffd160-11c4-4b0e-8185-cf29c729cf98/miniconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning:\n\nX does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n\n"}},"pos":41.994140625,"scrolled":true,"start":1657220796129,"state":"done","type":"cell"}
{"id":"00048b","input":"#df['music_genre_codes'] = df['music_genre'].map({'Electronic': 0, 'Anime': 1, 'Jazz': 2, 'Alternative': 3, 'Country': 4, 'Rap': 5, 'Blues': 6, 'Rock': 7, 'Classical': 8, 'Hip-Hop': 9})","kernel":"ds_env","pos":41.5,"state":"done","type":"cell"}
{"id":"5585a9","input":"# df[\"mode\"] = dummies[\"Major\"]\n# df.head()","kernel":"ds_env","pos":41,"state":"done","type":"cell"}
{"id":"bfc34a","input":"#df.drop(['key'], axis=1, inplace=True)\n#df.head()","kernel":"ds_env","pos":41.875,"state":"done","type":"cell"}
{"id":0,"time":1657219182424,"type":"user"}
{"last_load":1657202803998,"type":"file"}
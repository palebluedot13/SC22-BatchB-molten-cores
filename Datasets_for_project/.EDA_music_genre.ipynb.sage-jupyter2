{"backend_state":"init","connection_file":"/projects/afffd160-11c4-4b0e-8185-cf29c729cf98/.local/share/jupyter/runtime/kernel-0fd6d5a7-a5ab-4175-b87f-3a79360d7d85.json","kernel":"ds_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"13230c","input":"","pos":55,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"3e04f8","input":"","pos":57,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"54f4e0","input":"","pos":0,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9f17ab","input":"# Accuracy of the test\ngetScore(y_test,y_hat)","pos":32,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"becbad","input":"estimator = XGBClassifier(\n    objective= 'binary:logistic',\n    nthread=4,\n    seed=42\n)\nparameters = {\n    'max_depth': range (2, 5),\n    'n_estimators': range(100, 200)\n}\ngrid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid=parameters,\n    n_jobs = 10,\n    cv = 2,\n    verbose=True\n)\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_estimator_","pos":48,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"fdc234","input":"","pos":56,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"c34954","input":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\nimport sklearn\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport copy\nimport sklearn\nimport xgboost as xgb\nimport warnings\n\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom xgboost import XGBClassifier\nfrom IPython.display import Image\nfrom subprocess import call","pos":7,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"6476a0","input":"# evaluate a logistic regression model using k-fold cross-validation\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"73974d","input":"scatterplot = px.scatter(df.sort_values(by=[\"tempo\"]), x = \"loudness\", y = \"tempo\", color = \"music_genre_name\")\nscatterplot.update_layout(height=600, width=600, title_text=\"Scatter\")\nscatterplot.write_html(\"tempo_loudness_scatter\")","pos":24,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"0866a2","input":"#Building a model\nclf = DecisionTreeClassifier(max_depth = 2, random_state = 0)\n\n#Fitting in our data\nclf = clf.fit(X_train,y_train)\n\n#Predicting X_test\ny_pred = clf.predict(X_test)\n\n\n# prepare the cross-validation procedure\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\n# create model\nmodel = DecisionTreeClassifier()\n# evaluate model\nscores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n\nfig, axe = plt.subplots(figsize=(36,30))\ntree.plot_tree(clf, ax = axe, fontsize=15)","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy: 0.464 (0.006)\n"},"1":{"data":{"text/plain":"[Text(0.5, 0.8333333333333334, 'X[0] <= 0.214\\ngini = 0.9\\nsamples = 40000\\nvalue = [4019, 3992, 4004, 4004, 3979, 4023, 3988, 4016, 4028\\n3947]'),\n Text(0.25, 0.5, 'X[8] <= -1.256\\ngini = 0.852\\nsamples = 22128\\nvalue = [3379, 3912, 3145, 1590, 2460, 39, 3603, 49, 3801\\n150]'),\n Text(0.125, 0.16666666666666666, 'gini = 0.405\\nsamples = 3834\\nvalue = [47, 389, 348, 6, 22, 0, 110, 1, 2909, 2]'),\n Text(0.375, 0.16666666666666666, 'gini = 0.842\\nsamples = 18294\\nvalue = [3332, 3523, 2797, 1584, 2438, 39, 3493, 48, 892, 148]'),\n Text(0.75, 0.5, 'X[10] <= -0.148\\ngini = 0.826\\nsamples = 17872\\nvalue = [640, 80, 859, 2414, 1519, 3984, 385, 3967, 227, 3797]'),\n Text(0.625, 0.16666666666666666, 'gini = 0.814\\nsamples = 10207\\nvalue = [433, 52, 687, 1638, 1392, 1147, 327, 3437, 219, 875]'),\n Text(0.875, 0.16666666666666666, 'gini = 0.701\\nsamples = 7665\\nvalue = [207, 28, 172, 776, 127, 2837, 58, 530, 8, 2922]')]"},"exec_count":11,"output_type":"execute_result"},"2":{"data":{"image/png":"a0e926894cb318e98194b52c7a7959541cfb30ca","text/plain":"<Figure size 2592x2160 with 1 Axes>"},"exec_count":11,"output_type":"execute_result"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"7f484c","input":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [2,4]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","pos":43,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"8bbf1b","input":"warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n\nmdl5 = RandomForestClassifier(max_depth=2, random_state=0)   # RandomForestRegressor is added.\n\nrf_RandomGrid = RandomizedSearchCV(estimator = mdl5, param_distributions = param_grid, cv = 10, verbose=2, n_jobs = 4)\ny_pred6 = rf_RandomGrid.fit(X_train, y_train).predict(X_test)      # Fits the model\n\ny_pred5 = mdl5.fit(X_train, y_train).predict(X_test)\n\n\nprint(\"\\n------------------------------------------------------\\n\")\ngetScore(y_test, y_pred6)\nprint(\"\\n------------------------------------------------------\\n\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred6))\nprint(\"\\n------------------------------------------------------\\n\")","output":{"0":{"name":"stdout","output_type":"stream","text":"Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"},"1":{"name":"stdout","output_type":"stream","text":"\n------------------------------------------------------\n\n7.9972\n              precision    recall  f1-score   support\n\n Electronic:       0.48      0.65      0.55       981\n      Anime:       0.64      0.63      0.63      1008\n       Jazz:       0.53      0.27      0.36       996\nAlternative:       0.54      0.03      0.06       996\n    Country:       0.51      0.49      0.50      1021\n        Rap:       0.41      0.35      0.37       977\n      Blues:       0.50      0.36      0.42      1012\n       Rock:       0.40      0.82      0.54       984\n  Classical:       0.70      0.84      0.76       972\n    Hip-Hop:       0.49      0.63      0.55      1053\n\n    accuracy                           0.51     10000\n   macro avg       0.52      0.51      0.47     10000\nweighted avg       0.52      0.51      0.47     10000\n\n\n------------------------------------------------------\n\nAccuracy: 0.5069\n\n------------------------------------------------------\n\n"}},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"52b81d","input":"mat5 = confusion_matrix(y_test, y_pred6)\nrandom_confusion = sns.heatmap(mat5.T, square=True, annot=True, fmt='d',  xticklabels=labels, yticklabels=labels, cbar=True)\nplt.xlabel('Predicted label')\nplt.ylabel('True label');","output":{"0":{"data":{"image/png":"31e4277cd05ec5bd8fe205b1f01db55d3398a1cc","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":14,"output_type":"execute_result"}},"pos":45,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"ccbd84","input":"scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy: 0.444 (0.015)\n"}},"pos":38,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"b94ede","input":"print(X_train.shape)    # Prints out the shape of the variable.\nprint(X_test.shape)     # Prints out the shape of the variable.\nprint(y_train.shape)    # Prints out the shape of the variable.\nprint(y_test.shape)     # Prints out the shape of the variable.","output":{"0":{"name":"stdout","output_type":"stream","text":"(40000, 13)\n(10000, 13)\n(40000,)\n(10000,)\n"}},"pos":42,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"94afd0","input":"#This function returns the accuracy of tests\ndef getScore(y_test,y_hat):\n    labels = ['Electronic', 'Anime', 'Jazz', 'Alternative', 'Country', 'Rap', 'Blues', 'Rock', 'Classical', 'Hip-Hop']\n    print((np.sum((y_test - y_hat)**2))/len(y_test) )#mean squared error\n    print(classification_report(y_test, y_hat,target_names=[i+\":\" for i in labels]))\n\n#This function replaces labels, such as Jazz, Rock, Rap, etc. with numbers. Each label now carries it's own number\n\ndef stringToInt(dataFrame,col):\n    test = {}\n    for i in dict(enumerate(dataFrame[col].unique())).items():#is a dictionary of the keys and corespodening number\n        #makes it so the keys and values of the dictionary switch\n        test[i[1]]=i[0]\n    print(test,'\\n')\n    return dataFrame[col].map(test)","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"50ed78","input":"getScore(y_test, y_pred)","output":{"0":{"name":"stdout","output_type":"stream","text":"5.4102\n"},"1":{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n Electronic:       0.74      0.70      0.72       981\n      Anime:       0.83      0.78      0.81      1008\n       Jazz:       0.64      0.63      0.63       996\nAlternative:       0.57      0.51      0.53       996\n    Country:       0.69      0.65      0.67      1021\n        Rap:       0.46      0.51      0.48       977\n      Blues:       0.70      0.65      0.67      1012\n       Rock:       0.57      0.74      0.64       984\n  Classical:       0.87      0.85      0.86       972\n    Hip-Hop:       0.49      0.49      0.49      1053\n\n    accuracy                           0.65     10000\n   macro avg       0.66      0.65      0.65     10000\nweighted avg       0.66      0.65      0.65     10000\n\n"}},"pos":49,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"2d79bf","input":"df.to_csv('modified_music_genre.csv', index=False)","pos":52,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"1d9795","input":"new_df = pd.read_csv(\"modified_music_genre.csv\")\nnew_df.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>popularity</th>\n      <th>acousticness</th>\n      <th>danceability</th>\n      <th>duration_ms</th>\n      <th>energy</th>\n      <th>instrumentalness</th>\n      <th>key</th>\n      <th>liveness</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>tempo</th>\n      <th>valence</th>\n      <th>music_genre</th>\n      <th>music_genre_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>27.0</td>\n      <td>0.004680</td>\n      <td>0.652</td>\n      <td>270703.43</td>\n      <td>0.941</td>\n      <td>0.792</td>\n      <td>0</td>\n      <td>0.115</td>\n      <td>-5.201</td>\n      <td>0</td>\n      <td>0.0748</td>\n      <td>100.89</td>\n      <td>0.7590</td>\n      <td>0</td>\n      <td>Electronic</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.960000</td>\n      <td>0.160</td>\n      <td>322600.00</td>\n      <td>0.084</td>\n      <td>0.843</td>\n      <td>0</td>\n      <td>0.119</td>\n      <td>-21.254</td>\n      <td>1</td>\n      <td>0.0453</td>\n      <td>104.05</td>\n      <td>0.0695</td>\n      <td>8</td>\n      <td>Classical</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30.0</td>\n      <td>0.953000</td>\n      <td>0.188</td>\n      <td>157027.00</td>\n      <td>0.102</td>\n      <td>0.000</td>\n      <td>0</td>\n      <td>0.186</td>\n      <td>-18.123</td>\n      <td>1</td>\n      <td>0.0329</td>\n      <td>88.40</td>\n      <td>0.1530</td>\n      <td>8</td>\n      <td>Classical</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37.0</td>\n      <td>0.000052</td>\n      <td>0.432</td>\n      <td>263467.00</td>\n      <td>0.857</td>\n      <td>0.000</td>\n      <td>0</td>\n      <td>0.218</td>\n      <td>-3.024</td>\n      <td>0</td>\n      <td>0.0767</td>\n      <td>180.87</td>\n      <td>0.4350</td>\n      <td>1</td>\n      <td>Anime</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36.0</td>\n      <td>0.979000</td>\n      <td>0.341</td>\n      <td>410850.00</td>\n      <td>0.107</td>\n      <td>0.724</td>\n      <td>0</td>\n      <td>0.129</td>\n      <td>-19.950</td>\n      <td>1</td>\n      <td>0.0327</td>\n      <td>104.05</td>\n      <td>0.2130</td>\n      <td>8</td>\n      <td>Classical</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   popularity  acousticness  danceability  duration_ms  energy  \\\n0        27.0      0.004680         0.652    270703.43   0.941   \n1         0.0      0.960000         0.160    322600.00   0.084   \n2        30.0      0.953000         0.188    157027.00   0.102   \n3        37.0      0.000052         0.432    263467.00   0.857   \n4        36.0      0.979000         0.341    410850.00   0.107   \n\n   instrumentalness  key  liveness  loudness  mode  speechiness   tempo  \\\n0             0.792    0     0.115    -5.201     0       0.0748  100.89   \n1             0.843    0     0.119   -21.254     1       0.0453  104.05   \n2             0.000    0     0.186   -18.123     1       0.0329   88.40   \n3             0.000    0     0.218    -3.024     0       0.0767  180.87   \n4             0.724    0     0.129   -19.950     1       0.0327  104.05   \n\n   valence  music_genre music_genre_name  \n0   0.7590            0       Electronic  \n1   0.0695            8        Classical  \n2   0.1530            8        Classical  \n3   0.4350            1            Anime  \n4   0.2130            8        Classical  "},"exec_count":26,"output_type":"execute_result"}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"4a73c3","input":"param_grid = {'max_depth': np.arange(3, 6)}\n\ntree = GridSearchCV(DecisionTreeClassifier(), param_grid)\n\ntree.fit(X_train, y_train)\ntree_pred = tree.predict_proba(X_test)[:, 1]\n\n\nprint (\"DecisionTree: Area under the ROC curve = {}\".format(tree))","output":{"0":{"name":"stdout","output_type":"stream","text":"DecisionTree: Area under the ROC curve = GridSearchCV(estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': array([3, 4, 5])})\n"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"5fc9dd","input":"#getting dataframe\ndf = pd.read_csv(\"music_genre.csv\")\n\n#dropping columns\nbadInfo = [\"instance_id\",\"obtained_date\",\"artist_name\",\"track_name\"]\ndf.drop(columns=badInfo,axis=1,inplace=True)\n\n#dropping null rows\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True) #Very good practice to reset how your rows are counted when you drop rows.\n\n#fixing the problem with 'tempo' column\ndf[\"tempo\"]=df[\"tempo\"].replace(\"?\",np.nan)\ndf[\"tempo\"] = df[\"tempo\"].astype(\"float\")\ndf[\"tempo\"]=df.groupby(\"music_genre\")[\"tempo\"].transform(lambda x: x.fillna(x.mean(skipna=True)))\ndf['tempo'] = np.around(df['tempo'],2)\n\n#fixing the problem with 'duration' column\ndf[\"duration_ms\"]=df[\"duration_ms\"].replace(-1.0,np.nan)\ndf[\"duration_ms\"]=df.groupby(\"music_genre\")[\"duration_ms\"].transform(lambda x: x.fillna(x.mean(skipna=True)))\ndf['duration_ms'] = np.around(df['duration_ms'],2)\n\n#changing the values from string to int\ndf['key'] = stringToInt(df,'key')\ndf['mode'] = stringToInt(df,'mode')\ndf['music_genre_name'] = df['music_genre']\ndf['music_genre'] = stringToInt(df,'music_genre')\n\npio.templates.default = \"plotly_dark\"\nplt.style.use(\"dark_background\")\n\ndfxg = copy.deepcopy(df)","output":{"0":{"name":"stdout","output_type":"stream","text":"{'A#': 0, 'D': 1, 'G#': 2, 'C#': 3, 'F#': 4, 'B': 5, 'G': 6, 'F': 7, 'A': 8, 'C': 9, 'E': 10, 'D#': 11} \n\n{'Minor': 0, 'Major': 1} \n\n{'Electronic': 0, 'Anime': 1, 'Jazz': 2, 'Alternative': 3, 'Country': 4, 'Rap': 5, 'Blues': 6, 'Rock': 7, 'Classical': 8, 'Hip-Hop': 9} \n\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"9f2f8f","input":"print(\"hello\")","output":{"0":{"name":"stdout","output_type":"stream","text":"hello\n"}},"pos":58,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"893791","input":"#Setting X to be all the input columns \nX = df.loc[:,df.columns[:-2]]\n\n\n#Setting y to be the desired column\ny= df['music_genre']#what we want\n\n#Splitting test and train ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n#Scaling X variables\nsc = preprocessing.StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform (X_test)\n\n#for heat map\nlabels = ['Electronic', 'Anime', 'Jazz', 'Alternative', 'Country', 'Rap', 'Blues', 'Rock', 'Classical', 'Hip-Hop']","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"c94c1e","input":"\nfig = px.histogram(df.sort_values(by=[\"tempo\"]), x='popularity', y='tempo', color='music_genre_name',color_discrete_sequence=px.colors.qualitative.Dark24)\n\nfig.update_layout(height=600,width=600,title_text=\"Tempo Vs Popularity\")\n\n#fig.write_html(\"TempoVsPopularity.html\")\n\nfig","output":{"0":{"data":{"iframe":"60941e454b4c26799edddb9b0b7affe5568f64f8"},"exec_count":5,"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"6f6337","input":"\n#Building a model\nmodel = LogisticRegression()\n\n\n#Fitting our data\nmodel.fit(X_train, y_train)\n\n\n#Prediction of X_test\ny_pred = model.predict(X_test)\n\n\n#Finding out accurasy of train variables\nX_train_acc = model.score(X_train, y_train)\nprint(\"The Accuracy for Training Set is {}\".format(X_train_acc*100))\n\n\n#Finding out accuracy of test variables \ntest_acc = model.score(X_test, y_test)\nprint(\"The Accuracy for Test Set is {}\".format(test_acc*100))\n\n\n#Printing classification report\ngetScore(y_test,y_pred)\n","output":{"0":{"name":"stdout","output_type":"stream","text":"The Accuracy for Training Set is 53.1575\nThe Accuracy for Test Set is 53.059999999999995\n7.4106\n              precision    recall  f1-score   support\n\n Electronic:       0.59      0.60      0.59       981\n      Anime:       0.63      0.63      0.63      1008\n       Jazz:       0.48      0.41      0.44       996\nAlternative:       0.38      0.30      0.33       996\n    Country:       0.47      0.60      0.53      1021\n        Rap:       0.45      0.42      0.43       977\n      Blues:       0.51      0.46      0.48      1012\n       Rock:       0.51      0.65      0.57       984\n  Classical:       0.78      0.79      0.79       972\n    Hip-Hop:       0.48      0.46      0.47      1053\n\n    accuracy                           0.53     10000\n   macro avg       0.53      0.53      0.53     10000\nweighted avg       0.53      0.53      0.53     10000\n\n"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"f3ce32","input":"import plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\ndf_corr = df.corr() # Generate correlation matrix\nx = list(df_corr.columns)\ny = list(df_corr.index)\nz = np.array(df_corr)\n\nheat_map = ff.create_annotated_heatmap(\n    z,\n    x = x,\n    y = y ,\n    annotation_text = np.around(z, decimals=2),\n    hoverinfo='z'\n    )\n\nheat_map.update_layout(height=600,width=700,title_text=\"Heat Map\")\n\n#heat_map.write_html(\"heat_map.html\")","output":{"0":{"data":{"iframe":"a5fadb0ce6bf7aa6151ee6cd6fad3470437b4f92"},"exec_count":6,"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"05bdea","input":"#Confusion matrix\n\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g', xticklabels=labels, yticklabels=labels)","output":{"0":{"data":{"text/plain":"<AxesSubplot:>"},"exec_count":7,"output_type":"execute_result"},"1":{"data":{"image/png":"f3b7fab67c36dedf28b1d3ddc4e318bd3a1edb1e","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":7,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"c0ea89","input":"df = df.sort_values(by=[\"key\"])\npx.bar(df, x='music_genre_name', y='tempo',color='key')\n\n# tempo is highest for classical\n# tempo is lowest for electronic\n# clear pattern in tempo per genre\n#{'A#': 0, 'D': 1, 'G#': 2, 'C#': 3, 'F#': 4, 'B': 5, 'G': 6, 'F': 7, 'A': 8, 'C': 9, 'E': 10, 'D#': 11} ","output":{"0":{"data":{"iframe":"4813f12809b32b5e3d35173c11cd7ad670de0313"},"exec_count":7,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"302cb2","input":"#shows that we have an equal amount of each genre\n\ndf[\"music_genre_name\"].hist(figsize = (10, 10))","output":{"0":{"data":{"text/plain":"<AxesSubplot:>"},"exec_count":8,"output_type":"execute_result"},"1":{"data":{"image/png":"cc805b90cde527f48384ee6eda4008a8b58921a7","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":8,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"934ab6","input":"#Building a model\ngnb = GaussianNB()\n\n#Fitting data \ny_hat = gnb.fit(X_test, y_test).predict(X_test)\n\n#Confusion Matrix \nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g', xticklabels=labels, yticklabels=labels)","output":{"0":{"data":{"text/plain":"<AxesSubplot:>"},"exec_count":8,"output_type":"execute_result"},"1":{"data":{"image/png":"2dd6025449aa5d360a29b3104674097232dc1f97","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":8,"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"b856e5","input":"scatter_3d = px.scatter_3d(df, x = \"duration_ms\", y = \"loudness\", z = \"popularity\", size = \"energy\", color = \"music_genre_name\",size_max=50)\nscatter_3d.update_layout(height=600,width=600,title_text=\"3d Scatterplot\")\n\nscatter_3d.write_html(\"scatter_3d.html\")\nscatter_3d","output":{"0":{"data":{"iframe":"9979756be67914e4d748cba337a32c23f840c661"},"exec_count":9,"output_type":"execute_result"}},"pos":23,"scrolled":true,"type":"cell"}
{"cell_type":"markdown","id":"030963","input":"# ____________________________________________________________________________________________________\n\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"06bf11","input":"# ____________________________________________________________________________________________________\n\n","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"1d92f0","input":"# Music Genre Prediction: Molten Cores - GENUSX\n\n<img src=\"https://www.warcrafttavern.com/wp-content/uploads/2020/10/WoW-Classic-Molten-Core-Guide-1024x729.jpg\" alt=\"img\" width=\"85%\"/>\n\n","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"1e9d0d","input":"# ____________________________________________________________________________________________________\n\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"1ef4a5","input":"# ____________________________________________________________________________________________________","pos":50,"type":"cell"}
{"cell_type":"markdown","id":"2d11c1","input":"# ____________________________________________________________________________________________________\n\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"329a49","input":"## Plot\n\n","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"33a848","input":"# ____________________________________________________________________________________________________\n\n","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"36b999","input":"##### [Markdown Guide cheat Sheet](https://www.markdownguide.org/cheat-sheet/)\n\n","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"474e2e","input":"## Naive Bayes \n\nAbout  Naive Bayes :\nNative Bayes is a machine learning model for classification that uses the Bayes Theorem(\t<img src=\"https://s0.wp.com/latex.php?latex=%5Ctextrm%7BP%28H+%5Ctextbar+E%29+%3D+%7D+%C2%A0%5Cfrac%7B%5Ctextrm%7B+P%28E+%5Ctextbar+H%29+%2A+P%28H%29%7D%7D+%7B%5Ctextrm%7BP%28E%29%7D%7D&bg=ffffff&fg=000&s=0&c=20201002\" alt=\"img\" width=\"200\"/>\t)\n\n  -  P(H) is the probability of hypothesis H being true. This is known as the prior probability.\n  -  P(E) is the probability of the evidence(regardless of the hypothesis).\n  -  P(E|H) is the probability of the evidence given that hypothesis is true.\n  -  P(H|E) is the probability of the hypothesis given that the evidence is there.\n\nGood at  predicting:\n\n- Classical\n\nOkay at predicting:\n\n- Electronic\n- Anime\n- Hip-Hop\n\nBad at predicting:\n\n- everything else\n\n","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"50532f","input":"# ____________________________________________________________________________________________________\n\n","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"51ba4c","input":"## Notes:\n\n- ### Instrumentalness: vocals in a track\n\n- ### Speechiness: detects the pressents of vocal words in a track\n\n- ### Music genre: (10 different types)\n\t0. Electronic\n    1. Classical\n    2. Jazz\n    3. anime\n    4. Rock\n    5. country\n    6. Rap\n    7. Blues\n    8. Hip-Hop\n    9. Alternative\n\n# Problems with table:\n\n- [x] negative duration\n- [x] missing tempos\n- [x] rows 10000-10005 are nan values for every column\n- [ ] negative loudness\n\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"6c877e","input":"## Classification Trees\n\n","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"7761cb","input":"#### Export code of modified dataset\n","pos":51,"type":"cell"}
{"cell_type":"markdown","id":"8db7d8","input":"## Xgboost:\n\nXgBoost stands for Extreme Gradient Boosting\n\n","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"927e9f","input":"# ____________________________________________________________________________________________________","pos":59,"type":"cell"}
{"cell_type":"markdown","id":"92dffd","input":"# ____________________________________________________________________________________________________\n\n","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"a93d3f","input":"## All Libaries & Imports \n\n","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"b61414","input":"# ____________________________________________________________________________________________________","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"bc10b3","input":"# Functions\n\n","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"c14b2b","input":"# ____________________________________________________________________________________________________","pos":54,"type":"cell"}
{"cell_type":"markdown","id":"c530eb","input":"# ____________________________________________________________________________________________________\n\n","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"d3ff73","input":"## Spiting and scaling data\n\n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"e1dd26","input":"# **Random forest** \n\nRandom forest is a commonly-used machine learning algorithm trademarked by Leo Breiman and Adele Cutler, which combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.\n\n## **Benefits and challenges of random forest** \n\n#### **Benefits**\n\n- **Reduced risk of overfitting:** Decision trees run the risk of overfitting as they tend to tightly fit all the samples within training data. However, when there’s a robust number of decision trees in a random forest, the classifier won’t overfit the model since the averaging of uncorrelated trees lowers the overall variance and prediction error.\n  \n- **Provides flexibility:** Since random forest can handle both regression and classification tasks with a high degree of accuracy, it is a popular method among data scientists. Feature bagging also makes the random forest classifier an effective tool for estimating missing values as it maintains accuracy when a portion of the data is missing.\n\n- **Easy to determine feature importance:** Random forest makes it easy to evaluate variable importance, or contribution, to the model. There are a few ways to evaluate feature importance. Gini importance and mean decrease in impurity (MDI) are usually used to measure how much the model’s accuracy decreases when a given variable is excluded. However, permutation importance, also known as mean decrease accuracy (MDA), is another importance measure. MDA identifies the average decrease in accuracy by randomly permutating the feature values in oob samples.\n\n#### **Challenges**\n\n- **Time-consuming process:** Since random forest algorithms can handle large data sets, they can be provide more accurate predictions, but can be slow to process data as they are computing data for each individual decision tree.\n  \n- **Requires more resources:** Since random forests process larger data sets, they’ll require more resources to store that data.\n  \n- **More complex:** The prediction of a single decision tree is easier to interpret when compared to a forest of them.\n\n","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"ea10dc","input":"# Fixing problems with the Dataset\n\n","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"ef7213","input":"# ____________________________________________________________________________________________________\n\n","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"fb664e","input":"# ____________________________________________________________________________________________________\n\n","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"fc484d","input":"**About Classification Trees:** \n\nDecision Trees \\(DTs\\) are a non\\-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n\n**Pros**\n\n- Simple to understand and to interpret. Trees can be visualized.\n- Requires little data preparation.\n- Able to handle both numerical and categorical data.\n- Possible to validate a model using statistical tests.\n\n**Cons**\n\n- Decision\\-tree learners can create over\\-complex trees that do not generalize the data well.\n- Decision trees can be unstable because small var iations in the data might result in a completely different tree being generated\n- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure.\n- Decision tree learners create biased trees if some classes dominate.\n\n","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"ff8975","input":"# Logistic Regression \n\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable.\n\nIn logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).\n\n#### Types of LR\n\nWhen we talk about Logistic Regression in general, we usually mean Binary logistic regression, although there are other types of Logistic Regression as well.\n\nLogistic Regression can be divided into types based on the type of classification it does. With that in view, there are 3 types of Logistic Regression. Let’s talk about each of them:\n\n- Binary Logistic Regression\n- Multinomial Logistic Regression\n- Ordinal Logistic Regression\n\nIn our case, we used **Multinomial Logistic Regression**\n\nMultinomial Logistic Regression deals with cases when the target or independent variable has three or more possible values.\n\n**For example**, the use of Chest X-ray images as features that give indication about one of the three possible outcomes (No disease, Viral Pneumonia, COVID-19). The multinomial Logistic Regression will use the features to classify the example into one of the three possible outcomes in this case. There can of course be more than three possible values of the target variable. in our case, there are **10**.\n\n","pos":26,"type":"cell"}
{"id":0,"time":1657739341856,"type":"user"}
{"last_load":1657745777851,"type":"file"}